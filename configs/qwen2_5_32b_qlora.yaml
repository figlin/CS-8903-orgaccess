# Qwen 2.5 32B Fine-Tuning Configuration for OrgAccess RBAC
# Optimized for 2× H100 PCIe 80GB with 25,304 training examples
# Qwen2.5-32B is one of the strongest open models as of Oct 2024

model:
  base_model: "Qwen/Qwen2.5-32B-Instruct"  # Latest Qwen 2.5 instruction-tuned
  model_type: "qwen2"
  load_in_4bit: true
  device_map: "auto"  # Automatically distributes across GPUs

# QLoRA Configuration - 4-bit quantization for memory efficiency
qlora:
  use_qlora: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Hyperparameters - Optimized for 32B model with 25K examples
lora:
  r: 32                        # Good for large dataset (prevent overfitting)
  lora_alpha: 64               # Scaling factor (2x rank)
  lora_dropout: 0.1            # Higher dropout for regularization
  target_modules:              # Qwen2.5 attention and MLP layers
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Hyperparameters - Optimized for 2× H100 PCIe (multi-GPU)
training:
  num_train_epochs: 3          # Sufficient for 25K examples
  per_device_train_batch_size: 2  # Per GPU (2 GPUs × 2 = 4 total per step)
  per_device_eval_batch_size: 2   # Per GPU
  gradient_accumulation_steps: 4  # Effective batch = 2*2*4 = 16 (same as others)

  learning_rate: 5.0e-5        # LOWER than Gemma/Llama (32B needs careful tuning)
  lr_scheduler_type: "cosine"  # Cosine decay works well
  warmup_ratio: 0.05           # 5% warmup for stability
  weight_decay: 0.05           # Regularization
  max_grad_norm: 1.0           # Gradient clipping

  # Optimization
  optim: "paged_adamw_8bit"    # Memory-efficient optimizer for QLoRA
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8

  # Mixed precision - Qwen2.5 works excellently with bfloat16
  bf16: true                   # Use bfloat16 (H100 optimized)
  fp16: false

  # Logging and checkpointing
  logging_steps: 100           # Log every 100 steps
  save_strategy: "steps"       # Save checkpoints by steps
  save_steps: 500              # Save every 500 steps (~every 1/3 epoch)
  save_total_limit: 3          # Keep only 3 best checkpoints
  eval_strategy: "steps"       # Evaluate by steps
  eval_steps: 500              # Evaluate every 500 steps
  load_best_model_at_end: true # Load best checkpoint at end
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # Output
  output_dir: "./outputs/qwen2_5_32b_orgaccess_qlora"
  run_name: "qwen2.5-32b-orgaccess-qlora-v1"

  # Training stability
  seed: 42
  data_seed: 42

  # Early stopping (prevent overfitting)
  early_stopping_patience: 3       # Stop if no improvement for 3 eval checks
  early_stopping_threshold: 0.001  # Minimum improvement threshold

  # Multi-GPU settings (HuggingFace Trainer handles this automatically)
  ddp_find_unused_parameters: false  # Faster training

# Data Configuration
data:
  train_file: "training_data/train.parquet"
  val_file: "training_data/validation.parquet"
  max_seq_length: 2048         # Max sequence length (OrgAccess examples fit well)
  packing: false               # Don't pack (we want individual examples)

# Generation Config (for validation)
generation:
  max_new_tokens: 512          # Sufficient for response + rationale
  temperature: 0.1             # Lower temp for evaluation
  top_p: 0.9
  do_sample: false             # Greedy decoding for eval

# Resource limits
resources:
  max_memory_mb: 160000        # 160GB total (2× H100 PCIe 80GB each)

# Experiment tracking
wandb:
  enabled: false               # Set to true to enable WandB tracking
  project: "orgaccess-finetuning"
  entity: null                 # Your WandB username (if enabled)

notes: |
  Fine-tuning configuration for Qwen2.5-32B on OrgAccess RBAC benchmark.

  QWEN 2.5 ADVANTAGES (Released Oct 2024):
  - One of the strongest open models available
  - Exceptional reasoning and instruction-following
  - 32B parameters = significantly more capacity than 8B/12B models
  - Excels at complex decision-making tasks (perfect for RBAC!)
  - Strong performance on code and structured reasoning
  - Matches or exceeds GPT-3.5/Claude-2 on many benchmarks

  MULTI-GPU SETUP (2× H100 PCIe):
  - HuggingFace Trainer handles multi-GPU automatically (DDP)
  - Model sharded across 2 GPUs using device_map="auto"
  - Batch size: 2 per GPU × 2 GPUs = 4 samples per step
  - Gradient accumulation: 4 steps → effective batch = 16
  - ~2x faster than single H100 (~1.5-2 hours total)

  KEY DIFFERENCES FROM SMALLER MODELS:
  - Model size: 32B vs 12B (Gemma) or 8B (Llama) - ~2.7-4x larger
  - Learning rate: 5e-5 (LOWEST - large models are very sensitive)
  - Batch size: 2 per device (32B needs more VRAM than 12B)
  - Memory: ~40-50GB VRAM per GPU (2 GPUs needed)
  - Training time: ~1.5-2 hours on 2× H100 (vs ~2.5-3hrs for Gemma-12B on 1× H100)

  COMPARISON TO LLAMA-3.1-8B CONFIG:
  - Model size: 32B vs 8B parameters (4x larger)
  - GPUs: 2× H100 vs 1× A40
  - Learning rate: 5e-5 vs 1e-4 (2x lower for stability)
  - Total VRAM: ~80-100GB vs ~12-15GB
  - Expected performance: 65-70% accuracy vs 54.8% (Llama)

  Dataset:
  - Training: 25,304 examples (70% of Easy+Medium, 80% split)
  - Validation: 6,326 examples (70% of Easy+Medium, 20% split)
  - Test: 24,170 examples (30% Easy+Medium + 100% Hard)

  Task: 3-class classification for permission decisions:
  - full: All permissions satisfied
  - partial: Some permissions satisfied (collaboration/location)
  - rejected: Critical permission violations

  Training details:
  - Effective batch size: 16 (2 per GPU × 2 GPUs × 4 accumulation)
  - Steps per epoch: ~1,582 (25,304 / 16)
  - Total steps: ~4,746 (3 epochs)
  - Checkpoints: 9 (every 500 steps)
  - Evaluation: 9 times (every 500 steps)

  Expected training time on 2× H100 PCIe:
  - ~1.5-2 hours total (multi-GPU nearly 2x faster than single)
  - ~$6-7 on RunPod (2× $3.19/hr × 1.5-2hrs)
  - BEST performance/cost ratio for 32B model
  - Faster than single H100 SXM and cheaper than alternatives

  Memory requirements per GPU:
  - QLoRA with 4-bit: ~40-50GB VRAM per GPU
  - Requires: 2× H100 PCIe/SXM (80GB each) ✅ RECOMMENDED
  - Also works: 2× A100 80GB SXM (slower, similar cost)
  - Will NOT fit: Single GPU of any kind, A100 40GB

  Qwen2.5 Architecture Notes:
  - Transformer architecture with RoPE embeddings
  - SwiGLU activations (like Llama/Gemma)
  - Grouped-query attention (GQA) for efficiency
  - 128K context window (we only use 2K for this task)
  - Trained on high-quality multilingual data
  - Strong reasoning and math capabilities

  Performance Expectations:
  - Llama-3.1-8B achieved: 54.8% accuracy, 35.3% F1 (3.4x paper baseline!)
  - Gemma-3-12B target: 60-65% accuracy (larger model)
  - Qwen2.5-32B target: 65-70% accuracy (best reasoning + largest)
  - Paper baseline: 16% accuracy ± 7%
  - Goal: 4-4.5x improvement over paper baseline

  Why Qwen2.5-32B is worth the cost:
  - Best-in-class reasoning for complex decisions
  - Likely to achieve highest accuracy on hard split
  - Excellent for publication-quality results
  - Only ~$2-3 more than Gemma-12B training
  - SOTA open model performance (competitive with GPT-3.5)

  Multi-GPU Training Notes:
  - Run with: python scripts/train_qlora.py --config configs/qwen2_5_32b_qlora.yaml
  - HuggingFace Trainer auto-detects GPUs via torch.cuda.device_count()
  - Uses DistributedDataParallel (DDP) automatically
  - Model sharding handled by device_map="auto"
  - No code changes needed - it just works!
