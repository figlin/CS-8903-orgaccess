# Gemma 3 12B Fine-Tuning Configuration for OrgAccess RBAC
# Optimized for H100 PCIe 80GB with 25,304 training examples
# Based on successful Llama-3.1-8B config but adjusted for Gemma-3's architecture

model:
  base_model: "google/gemma-3-12b-it"  # Latest Gemma 3 instruction-tuned version
  model_type: "gemma"
  load_in_4bit: true
  device_map: "auto"

# QLoRA Configuration - 4-bit quantization for memory efficiency
qlora:
  use_qlora: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Hyperparameters - Optimized for 12B model with 25K examples
lora:
  r: 32                        # Same as Llama config (good for 12B with large dataset)
  lora_alpha: 64               # Scaling factor (2x rank)
  lora_dropout: 0.1            # Higher dropout for regularization
  target_modules:              # Gemma-2 specific attention modules
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Hyperparameters - Optimized for H100 PCIe 80GB
training:
  num_train_epochs: 3          # Same as Llama (sufficient for 25K examples)
  per_device_train_batch_size: 4  # OPTIMIZED for H100 80GB (was 2 for A40/A100)
  per_device_eval_batch_size: 4   # Match training batch size
  gradient_accumulation_steps: 4  # REDUCED from 8 (effective batch = 4*4 = 16, same total)

  learning_rate: 8.0e-5        # SLIGHTLY LOWER than Llama (larger models need smaller LR)
  lr_scheduler_type: "cosine"  # Cosine decay works well
  warmup_ratio: 0.05           # 5% warmup for stability
  weight_decay: 0.05           # Same regularization as Llama
  max_grad_norm: 1.0           # Gradient clipping

  # Optimization
  optim: "paged_adamw_8bit"    # Memory-efficient optimizer for QLoRA
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8

  # Mixed precision - Gemma-2 works well with bfloat16
  bf16: true                   # Use bfloat16 (recommended for A40/A100/H100)
  fp16: false

  # Logging and checkpointing (same as Llama)
  logging_steps: 100           # Log every 100 steps
  save_strategy: "steps"       # Save checkpoints by steps
  save_steps: 500              # Save every 500 steps (~every 1/3 epoch)
  save_total_limit: 3          # Keep only 3 best checkpoints
  eval_strategy: "steps"       # Evaluate by steps
  eval_steps: 500              # Evaluate every 500 steps
  load_best_model_at_end: true # Load best checkpoint at end
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # Output
  output_dir: "./outputs/gemma3_12b_orgaccess_qlora"
  run_name: "gemma3-12b-orgaccess-qlora-v1"

  # Training stability
  seed: 42
  data_seed: 42

  # Early stopping (prevent overfitting)
  early_stopping_patience: 3       # Stop if no improvement for 3 eval checks
  early_stopping_threshold: 0.001  # Minimum improvement threshold

# Data Configuration
data:
  train_file: "training_data/train.parquet"
  val_file: "training_data/validation.parquet"
  max_seq_length: 2048         # Max sequence length (OrgAccess examples fit well)
  packing: false               # Don't pack (we want individual examples)

# Generation Config (for validation)
generation:
  max_new_tokens: 512          # Sufficient for response + rationale
  temperature: 0.1             # Lower temp for evaluation
  top_p: 0.9
  do_sample: false             # Greedy decoding for eval

# Resource limits
resources:
  max_memory_mb: 80000         # 80GB for H100 PCIe

# Experiment tracking
wandb:
  enabled: false               # Set to true to enable WandB tracking
  project: "orgaccess-finetuning"
  entity: null                 # Your WandB username (if enabled)

notes: |
  Fine-tuning configuration for Gemma-3 12B on OrgAccess RBAC benchmark.

  GEMMA-3 IMPROVEMENTS (Released Oct 2024):
  - Enhanced reasoning capabilities vs Gemma-2
  - Better instruction following
  - Improved performance on complex decision-making tasks
  - Same architecture family, compatible with Gemma-2 configs

  KEY DIFFERENCES FROM LLAMA-3.1-8B CONFIG:
  - Model size: 12B vs 8B parameters (~50% larger)
  - GPU: H100 PCIe 80GB (vs A40 48GB for Llama-8B)
  - Batch size: 4 per device (SAME as Llama, thanks to H100's extra VRAM)
  - Gradient accumulation: 4 (SAME as Llama)
  - Learning rate: SLIGHTLY LOWER 1e-4 → 8e-5 (larger models are more sensitive)
  - Memory requirement: ~25-30GB VRAM (vs ~12-15GB for Llama-8B)

  COMPARISON TO ORIGINAL ENRON CONFIG (240 examples):
  - Dataset: 100x larger (25,304 vs 240)
  - LoRA rank: 32 (same - good for large dataset)
  - Learning rate: 8e-5 (vs 3e-4 for Enron - much slower)
  - Epochs: 3 (vs 10 for Enron - less overfitting risk)
  - Batch size: Smaller per-device but same effective size via accumulation

  Dataset:
  - Training: 25,304 examples (70% of Easy+Medium, 80% split)
  - Validation: 6,326 examples (70% of Easy+Medium, 20% split)
  - Test: 24,170 examples (30% Easy+Medium + 100% Hard)

  Task: 3-class classification for permission decisions:
  - full: All permissions satisfied
  - partial: Some permissions satisfied (collaboration/location)
  - rejected: Critical permission violations

  Training details:
  - Effective batch size: 16 (4 per device × 4 accumulation)
  - Steps per epoch: ~1,582 (25,304 / 16)
  - Total steps: ~4,746 (3 epochs)
  - Checkpoints: 9 (every 500 steps)
  - Evaluation: 9 times (every 500 steps)

  Expected training time on H100 PCIe:
  - ~2.5-3 hours total (H100 is ~2.5x faster than A40)
  - ~$8-10 on RunPod (~$3.19/hr)
  - FASTER and CHEAPER than A100 PCIe ($15) or A100 SXM ($13)

  Memory requirements:
  - QLoRA with 4-bit: ~25-30GB VRAM (batch_size=4)
  - Recommended: H100 PCIe/SXM (80GB) - BEST performance/cost
  - Works on: A100 80GB SXM (slower, similar cost)
  - Will NOT fit: A40 48GB, A100 40GB, RTX 4090 24GB (need batch_size=2)

  Gemma-3 Architecture Notes:
  - Uses Rotary Position Embeddings (RoPE) like Llama
  - SwiGLU activations
  - Grouped-query attention (GQA)
  - Enhanced reasoning over Gemma-2 (especially for complex decisions)
  - Instruction-tuned version (`-it`) recommended for chat format
  - Released October 2024 as Google's latest open model

  Performance Expectations:
  - Llama-3.1-8B achieved: 54.8% accuracy, 35.3% F1 on hard split (3.4x paper baseline!)
  - Gemma-3-12B target: 60-65% accuracy (larger + better reasoning)
  - Paper baseline: 16% accuracy ± 7%
  - Goal: 4x improvement over paper baseline
