# Llama 3.1 8B Fine-Tuning Configuration for OrgAccess RBAC
# Optimized for 25,304 training examples with permission reasoning

model:
  base_model: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  model_type: "llama"
  load_in_4bit: true
  device_map: "auto"

# QLoRA Configuration - 4-bit quantization for memory efficiency
qlora:
  use_qlora: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Hyperparameters - Optimized for 25K examples
lora:
  r: 32                        # LoRA rank (lower for larger dataset - less overfitting)
  lora_alpha: 64               # Scaling factor (2x rank)
  lora_dropout: 0.1            # Higher dropout for regularization with more data
  target_modules:              # Apply LoRA to attention and MLP layers
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Hyperparameters - Optimized for 25K examples (100x more than Enron!)
training:
  num_train_epochs: 3          # Fewer epochs needed with more data (was 10 for 240 examples)
  per_device_train_batch_size: 4  # Batch size per GPU
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size = 4*4 = 16

  learning_rate: 1.0e-4        # LOWER LR for larger dataset (was 3e-4 for 240 examples)
  lr_scheduler_type: "cosine"  # Cosine decay works well
  warmup_ratio: 0.05           # 5% warmup for better stability with larger dataset
  weight_decay: 0.05           # INCREASED weight decay to prevent overfitting
  max_grad_norm: 1.0           # Gradient clipping

  # Optimization
  optim: "paged_adamw_8bit"    # Memory-efficient optimizer for QLoRA
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8

  # Mixed precision
  bf16: true                   # Use bfloat16 if available (recommended for A40/A100)
  fp16: false

  # Logging and checkpointing (adjusted for 25K examples)
  logging_steps: 100           # Log every 100 steps (was 2 for 240 examples)
  save_strategy: "steps"       # Save checkpoints by steps
  save_steps: 500              # Save every 500 steps (~every 1/3 epoch)
  save_total_limit: 3          # Keep only 3 best checkpoints
  eval_strategy: "steps"       # Evaluate by steps
  eval_steps: 500              # Evaluate every 500 steps (~every 1/3 epoch)
  load_best_model_at_end: true # Load best checkpoint at end
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # Output
  output_dir: "./outputs/llama3_1_8b_orgaccess_qlora"
  run_name: "llama3.1-8b-orgaccess-qlora-v1"

  # Training stability
  seed: 42
  data_seed: 42

  # Early stopping (recommended for larger datasets to prevent overfitting)
  early_stopping_patience: 3       # Stop if no improvement for 3 eval checks
  early_stopping_threshold: 0.001  # Minimum improvement threshold

# Data Configuration
data:
  train_file: "training_data/train.parquet"
  val_file: "training_data/validation.parquet"
  max_seq_length: 2048         # Max sequence length (OrgAccess examples fit well)
  packing: false               # Don't pack (we want individual examples)

# Generation Config (for validation)
generation:
  max_new_tokens: 512          # Sufficient for response + rationale
  temperature: 0.1             # Lower temp for evaluation
  top_p: 0.9
  do_sample: false             # Greedy decoding for eval

# Resource limits
resources:
  max_memory_mb: 40000         # 40GB for A40/A100

# Experiment tracking
wandb:
  enabled: false               # Set to true to enable WandB tracking
  project: "orgaccess-finetuning"
  entity: null                 # Your WandB username (if enabled)

notes: |
  Fine-tuning configuration for Llama 3.1 8B on OrgAccess RBAC benchmark.

  KEY DIFFERENCES FROM ENRON CONFIG (240 examples):
  - Dataset: 100x larger (25,304 vs 240)
  - LoRA rank: REDUCED 96 → 32 (less capacity needed with more data)
  - Learning rate: REDUCED 3e-4 → 1e-4 (slower learning for stability)
  - Weight decay: INCREASED 0.01 → 0.05 (more regularization)
  - Dropout: INCREASED 0.08 → 0.1 (prevent overfitting)
  - Epochs: REDUCED 10 → 3 (less repetition needed)
  - Early stopping: ENABLED (wasn't needed for 240 examples)
  - Logging: REDUCED frequency (100 steps vs 2)

  Dataset:
  - Training: 25,304 examples (70% of Easy+Medium, 80% split)
  - Validation: 6,326 examples (70% of Easy+Medium, 20% split)
  - Test: 24,170 examples (30% Easy+Medium + 100% Hard)

  Task: 3-class classification for permission decisions:
  - full: All permissions satisfied
  - partial: Some permissions satisfied (collaboration/location)
  - rejected: Critical permission violations

  Training details:
  - Effective batch size: 16 (4 per device × 4 accumulation)
  - Steps per epoch: ~1,582 (25,304 / 16)
  - Total steps: ~4,746 (3 epochs)
  - Checkpoints: 9 (every 500 steps)
  - Evaluation: 9 times (every 500 steps)

  Expected training time on A40:
  - ~4-6 hours total
  - ~$3-5 on RunPod

  Memory requirements:
  - QLoRA with 4-bit: ~12-15GB VRAM
  - Recommended: A40 (48GB) or RTX 4090 (24GB)
